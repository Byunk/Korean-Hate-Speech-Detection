{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_RVJlO-Chd-o",
        "outputId": "a95282fa-b19d-4871-dd77-7be32e3e0c44"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /root/anaconda3/lib/python3.9/site-packages (4.23.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /root/anaconda3/lib/python3.9/site-packages (from transformers) (0.10.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /root/anaconda3/lib/python3.9/site-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /root/anaconda3/lib/python3.9/site-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /root/anaconda3/lib/python3.9/site-packages (from transformers) (0.13.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /root/anaconda3/lib/python3.9/site-packages (from transformers) (2022.7.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /root/anaconda3/lib/python3.9/site-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /root/anaconda3/lib/python3.9/site-packages (from transformers) (1.23.4)\n",
            "Requirement already satisfied: requests in /root/anaconda3/lib/python3.9/site-packages (from transformers) (2.28.1)\n",
            "Requirement already satisfied: filelock in /root/anaconda3/lib/python3.9/site-packages (from transformers) (3.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /root/anaconda3/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.3.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /root/anaconda3/lib/python3.9/site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /root/anaconda3/lib/python3.9/site-packages (from requests->transformers) (1.26.11)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /root/anaconda3/lib/python3.9/site-packages (from requests->transformers) (3.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /root/anaconda3/lib/python3.9/site-packages (from requests->transformers) (2022.9.14)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in /root/anaconda3/lib/python3.9/site-packages (from requests->transformers) (2.0.4)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import gc\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# seed 값 설정\n",
        "torch.manual_seed(555)\n",
        "\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "import transformers\n",
        "from transformers import AdamW\n",
        "\n",
        "from tqdm import tqdm\n",
        "import warnings\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_iA-6Qii1Q2y",
        "outputId": "83cf3fba-e29c-4d12-edba-da55ac6a7b44"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tokenizers in /root/anaconda3/lib/python3.9/site-packages (0.13.1)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install tokenizers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "MHgsgXZQ0ow9"
      },
      "outputs": [],
      "source": [
        "from tokenizers import (\n",
        "    decoders,\n",
        "    models,\n",
        "    normalizers,\n",
        "    pre_tokenizers,\n",
        "    processors,\n",
        "    trainers,\n",
        "    Tokenizer,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "YYkHv45P09Be"
      },
      "outputs": [],
      "source": [
        "tokenizer = Tokenizer(models.Unigram())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "HYOTfA8e1KuH"
      },
      "outputs": [],
      "source": [
        "from tokenizers import Regex\n",
        "\n",
        "tokenizer.normalizer = normalizers.Sequence(\n",
        "    [\n",
        "        normalizers.Replace(\"``\", '\"'),\n",
        "        normalizers.Replace(\"''\", '\"'),\n",
        "        normalizers.NFKD(),\n",
        "        normalizers.StripAccents(),\n",
        "        normalizers.Replace(Regex(\" {2,}\"), \" \"),\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "bvGasGb31Y18"
      },
      "outputs": [],
      "source": [
        "tokenizer.pre_tokenizer = pre_tokenizers.Metaspace()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3hOuVvYq1ZVL",
        "outputId": "f44de9cd-607a-44b2-f140-00afa695284f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[(\"▁Let's\", (0, 5)),\n",
              " ('▁test', (5, 10)),\n",
              " ('▁the', (10, 14)),\n",
              " ('▁pre-tokenizer!', (14, 29))]"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.pre_tokenizer.pre_tokenize_str(\"Let's test the pre-tokenizer!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "qZhbvulP1h6E"
      },
      "outputs": [],
      "source": [
        "special_tokens = [\"<cls>\", \"<sep>\", \"<unk>\", \"<pad>\", \"<mask>\", \"<s>\", \"</s>\"]\n",
        "trainer = trainers.UnigramTrainer(\n",
        "    vocab_size=25000, special_tokens=special_tokens, unk_token=\"<unk>\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "yJk7WRmz1jgp"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "tokenizer.model = models.Unigram()\n",
        "tokenizer.train([\"/root/team26/korean-hate-speech/unlabeled/unlabeled_comments.txt\"], trainer=trainer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ILi2xev42OTy",
        "outputId": "8550a52f-d191-42ee-db41-10331ee3be0c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['▁L', 'et', \"'\", 's', '▁', 'te', 'st', '▁th', 'is', '▁to', 'k', 'en', 'i', 'z', 'er', '.']\n"
          ]
        }
      ],
      "source": [
        "encoding = tokenizer.encode(\"Let's test this tokenizer.\")\n",
        "print(encoding.tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aqJZAYMH2Rsj",
        "outputId": "ea21bb91-e4f7-430a-f8af-b0d14bad30f9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 1\n"
          ]
        }
      ],
      "source": [
        "cls_token_id = tokenizer.token_to_id(\"<cls>\")\n",
        "sep_token_id = tokenizer.token_to_id(\"<sep>\")\n",
        "print(cls_token_id, sep_token_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6dq-aylo2UIP"
      },
      "outputs": [],
      "source": [
        "tokenizer.post_processor = processors.TemplateProcessing(\n",
        "    single=\"$A:0 <sep>:0 <cls>:2\",\n",
        "    pair=\"$A:0 <sep>:0 $B:1 <sep>:1 <cls>:2\",\n",
        "    special_tokens=[(\"<sep>\", sep_token_id), (\"<cls>\", cls_token_id)],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EQWUotu42Yxy",
        "outputId": "59bf2d1c-9c56-480f-ecdf-5cd9b1eefb75"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['▁L', 'et', \"'\", 's', '▁', 'te', 'st', '▁th', 'is', '▁to', 'k', 'en', 'i', 'z', 'er', '...', '<sep>', '▁', 'on', '▁a', '▁', 'pa', 'ir', '▁o', 'f', '▁s', 'ent', 'en', 'ce', 's', '!', '<sep>', '<cls>']\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2]\n"
          ]
        }
      ],
      "source": [
        "encoding = tokenizer.encode(\"Let's test this tokenizer...\", \"on a pair of sentences!\")\n",
        "print(encoding.tokens)\n",
        "print(encoding.type_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2XaP_77n2b4z"
      },
      "outputs": [],
      "source": [
        "tokenizer.decoder = decoders.Metaspace()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9gM5YVIK2gAL",
        "outputId": "83501bc3-d2e0-48d5-bca1-61ab84ea1312"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('my_tokenizer/tokenizer_config.json',\n",
              " 'my_tokenizer/special_tokens_map.json',\n",
              " 'my_tokenizer/tokenizer.json')"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import transformers\n",
        "from transformers import XLNetTokenizerFast\n",
        "from transformers import XLMRobertaTokenizerFast\n",
        "\n",
        "wrapped_tokenizer2 = XLMRobertaTokenizerFast(tokenizer_object=tokenizer)\n",
        "wrapped_tokenizer2.save_pretrained(\"my_xlmr\")\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
